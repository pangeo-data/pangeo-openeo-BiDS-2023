{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {},
   "source": [
    "# Data chunking with zarr and kerchunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bb79b-1b06-42c9-98d5-6185252c86e5",
   "metadata": {},
   "source": [
    "## Authors & Contributors\n",
    "### Authors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "- Pier Lorenzo Marasco, Ispra (Italy), [@pl-marasco](https://github.com/pl-marasco)\n",
    "\n",
    "### Contributors\n",
    "- Anne Fouilloux, University of Oslo (Norway), [@annefou](https://github.com/annefou)\n",
    "- Guillaume Eynard-Bontemps, CNES (France), [@guillaumeeb](https://github.com/guillaumeeb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>Why do chunking matter?</li>\n",
    "        <li>How can I read datasets by chunks to optimize memory usage?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about chunking</li>\n",
    "        <li>Learn about zarr </li>\n",
    "        <li>Use kerchunk to consolidate chunk metadata and prepare single ensemble datasets for parallel computing</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "When dealing with large data files or collections, it's often impossible to load all the data you want to analyze into a single computer's RAM at once. This is a situation where the Pangeo ecosystem can help you a lot. Xarray offers the possibility to work lazily on data __chunks__, which means pieces of an entire dataset. By reading a dataset in __chunks__ we can process our data piece by piece on a single computer and even on a distributed computing cluster using Dask (Cloud or HPC for instance).\n",
    "\n",
    "How we will process these 'chunks' in a parallel environment will be discussed in [dask_introduction](./dask_introduction.ipynb). The concept of __chunk__ will be explained here.\n",
    "\n",
    "When we process our data piece by piece, it's easier to have our input or ouput data also saved in __chunks__. [Zarr](https://zarr.readthedocs.io/en/stable/) is the reference library in the Pangeo ecosystem to save our Xarray multidimentional datasets in __chunks__.\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/) is not the only file format which uses __chunk__. We will also be using [kerchunk library](https://fsspec.github.io/kerchunk/) in this notebook to build a virtual __chunked__ dataset based on NetCDF files, and show how it optimizes the access and analysis of large datasets.\n",
    "\n",
    "The analysis is very similar to what we have done in previous episodes, however we will use data on a global coverage and not only on a small geographical area (e.g. Lombardia).\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using Global Long Term Statistics (1999-2019) products provided by the [Copernicus Global Land Service](https://land.copernicus.eu/global/index.html) and access them through [S3-comptabile storage](https://en.wikipedia.org/wiki/Amazon_S3) ([OpenStack Object Storage \"Swift\"](https://wiki.openstack.org/wiki/Swift)) with a data catalog we have created and made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c600794-dd9e-400b-bd09-dbb6e7039dad",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following main Python packages:\n",
    "\n",
    "- fsspec {cite:ps}`d-fsspec-2018`\n",
    "- s3fs {cite:ps}`d-s3fs-2016`\n",
    "- xarray {cite:ps}`d-xarray-hoyer2017` with [`netCDF4`](https://pypi.org/project/h5netcdf/) and [`h5netcdf`](https://pypi.org/project/h5netcdf/) engines\n",
    "- dask {cite:ps}`d-dask-2016`\n",
    "- kerchunk {cite:ps}`d-kerchunk-2021`\n",
    "- geopandas {cite:ps}`d-geopandas-jordahl2020`\n",
    "- matplotlib {cite:ps}`d-matplotlib-Hunter2007`\n",
    "\n",
    "Please install these packages if not already available in your Python environment (see [Setup page](https://pangeo-data.github.io/foss4g-2022/before/setup.html)).\n",
    "\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb32e7-cb5f-47cd-be1b-ec668266bec5",
   "metadata": {},
   "source": [
    "## title here\n",
    "\n",
    "we are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab40279-d857-437d-9405-3a0dfc9aa5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import stackstac\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85914868-6215-43ae-88d2-fbec027f9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aoi = gpd.read_file('../data/catchment_outline.geojson', crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(aoi.iloc[0].geometry)\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    intersects=aoi_geojson,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2019-02-01/2019-06-10\"\n",
    ").item_collection()\n",
    "sentinel2_l2a = stackstac.stack(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5711c-669d-418c-a0e4-f8e0beb8b6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentinel2_l2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d2938-d0ea-41fe-b32c-af81a8055416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is a __chunk__\n",
    "\n",
    "If you look carefully to `sentinel2_l2a`,  xarray.DataArray is a `dask.array` with a chunk size of `(1, 1, 1024, 1024)`. The full data would load arrays of dimensions `(101, 32, 20982, 10980)`, 746 592 of the 'chunk', in total 5.42 TiB into the computer's RAM.  \n",
    "\n",
    "We can see the Dask Array information by clicking the icon as circled blue in the image below.\n",
    "\n",
    "![Dask.array](../figures/datasize.png)\n",
    "\n",
    "By clicking Red circled triangle icon, we can have detailed informations on the xarray, such as Coordinates, Indexes and Attributes.   \n",
    "\n",
    "When you create Xarray object using stackstac, we can easily turns STAC collection into a lazy xarray.DataArray, in chunk form, so then it is backed by dask. \n",
    "\n",
    "The size and shape of chunk which we will use defines the parallelisation done by Dask, thus Picking a good chunksize will have significant effects on performance.  \n",
    "\n",
    "\n",
    "This is where understanding and using chunking correctly comes into play.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7dfb0c-5daa-41e8-bbb8-f453be29e1d7",
   "metadata": {},
   "source": [
    "__Chunking__ is splitting a dataset into small pieces. \n",
    "\n",
    "Original dataset, in one piece,  \n",
    "<img src=\"https://github.com/pangeo-data/pangeo-openeo-BiDS-2023/tree/main/tutorial/figures/notchunked.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "and we split it into several smaller pieces.  \n",
    "<img src=\"https://github.com/pangeo-data/pangeo-openeo-BiDS-2023/tree/main/tutorial/figures/chunked.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "We split it into pieces so that we can process our data block by block or __chunk__ by __chunk__.\n",
    "\n",
    "In our case, for the moment, we used stackstac without specifying 'chunk' explicitly.   The dataset is composed of 8MiB each, each contains, 1 time step, 1 band, 1024 x 1024 on x and y direction.  \n",
    "<img src=\"https://github.com/pangeo-data/pangeo-openeo-BiDS-2023/tree/main/tutorial/figures/chunk_original.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "\n",
    "If we have too small chunk size, we will divide our work flow in too small pieces, which can create too many communications, too many 'distirbution' overheads. \n",
    "If we have too big chunk size, we may not be able to hold the enough memory and our workflow may die.  \n",
    "\n",
    "The right size of chunk depends on your computation and the machine you use.  \n",
    "\n",
    "Here, 8MiB, is very small compare to usual RAM size available. For example, dask's default array size is 128MiB.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3cf9c-583b-46a8-b6eb-38083af17bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.get('array.chunk-size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a95b34-8ecb-4dca-8c4a-861d9b078012",
   "metadata": {},
   "source": [
    "## Modifying chunks\n",
    "Lets try to modify our chunk size.    \n",
    "\n",
    "\n",
    "To modify chunks on your existing Xarray  `DataArray`  we can use `chunk` function of Xarray.\n",
    "We know that we only need 3 bands for computing the snow index example, so we select only 'green','swir16','scl' to simplyfy our example.  \n",
    "\n",
    "We would like to have each time series separeated in each chunk, then keep all band informnation on one chunk, and let dask to compute x and y coordinate's chunk size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35188d-fe2d-4977-aa3d-7a0c49a2d75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentinel2_l2a=sentinel2_l2a.sel(\n",
    "    band=['green','swir16','scl']).chunk(\n",
    "    chunks={'time': 1, 'band':3, 'x':'auto','y':'auto'})\n",
    "sentinel2_l2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f05ca7-1439-4a44-aad5-0dda313eeb5f",
   "metadata": {},
   "source": [
    "If you look into details of any variable in the representation above, you'll see that each x and y coordinate's each 'chunk' is bigger, and we have much less chunks (6666 chunks) than example before.  Chunk size as 96 MiB,  is already more manageable than 8MiB small chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab2142-9c78-465c-b743-3028c10cb0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentinel2_l2a.chunk(chunks = ( 1, -1, 12048,2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac448da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go Further</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    You can try to apply different ways for specifying chunk.\n",
    "    <ul>\n",
    "        <li> chunks = -1 -> the entire array will be used as a single chunk\n",
    "        <li> chunks = {'x':-1, 'y': 1000} -> chunks of entire _x_ dimension, but splitted every 1000 values on _y_ dimension</li>\n",
    "        <li> chunks = {'x':-1, 'y': 'auto'} -> Xarray relies on Dask to use an ideal size according to the preferred chunk sizes for _y_ dimension</li>\n",
    "        <li> chunks = { 'x':-1 ,'y':\"500MiB\" } -> Xarray seeks the size according to a specific memory target expressed in MiB</li>\n",
    "        <li> chunks = ( 1, 3, 12048,2048) -> Specifying chunk size in the order of dimension. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f923b-6e45-4494-af6c-c537e1791f99",
   "metadata": {},
   "source": [
    "## Defining the chunk at the creatioin of Xarray\n",
    "\n",
    "We can define the chunk size when we create the object.  \n",
    "This is usually done with Xarray using the `chunks` kwarg when opening a file with `xr.open_dataset` or with `xr.open_mfdataset`, if you create Xarray from your local file.  \n",
    "In our snow index example, we create Xarray from stackstac. As stackstac's default 'chunksize' definition is 1024 for x and y dimension, we had that chunksize.  We can pass the chunksize option to stdeackstac and make that bigger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fd516-7366-4b22-9a6a-87ad615da69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sentinel2_l2a = stackstac.stack(items\n",
    "                                ,assets=['green','swir16','scl']\n",
    "                               ,chunksize=( 1, 3, 2048,2048)\n",
    ")\n",
    "sentinel2_l2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df00ba1-46ea-46a6-92e6-433187e8f433",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## So, why chunks?\n",
    "\n",
    "Chunks are mandatory for accessing files or dataset that are bigger than a single computer's memory. If all the data has to be accessed, it can be done sequentially e.g. chunks are processed one after the othe).\n",
    "\n",
    "Moreover, chunks allow for distributed processing and so increased speed for your data analysis, as seen in the next episode.\n",
    "\n",
    "### Chunks and files\n",
    "\n",
    "Xarray chunking possibilities also relies on the underlying input or output file format used. Most modern file format allows to store a dataset or a single file using chunks. NetCDF4 uses chunks when storing a file on the disk through the use of HDF5. Any read of data in a NetCDF4 file will lead to the load of at least one chunk of this file. So when reading one of its chunk as defined in `open_dataset` call, Xarray will take advantage of native file chunking and won't have to read the entire file too.\n",
    "\n",
    "\n",
    "Yet, it is really important to note that __Xarray chunks and file chunks are not necessarily the same__. It is however a really good idea to configure Xarray chunks so that they align well on input file format chunks (so ideally, Xarray chunks should contain one or several input file chunks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a0ddb-3b89-4392-9822-dfaa80111c52",
   "metadata": {},
   "source": [
    "## Zarr storage format\n",
    "\n",
    "This brings to our next subjects [Zarr](https://zarr.readthedocs.io/en/stable/) and [Kerchunk](https://fsspec.github.io/kerchunk/).\n",
    "\n",
    "If we can have our original dataset already 'chunked' and accessed in an optimized way according to it's actual byte storage on disk, we won't need to load entire dataset every time, and our data anlayzis, even working on the entire dataset, will be greatly optimized.\n",
    "\n",
    "Let's convert our intermediate data into Zarr format so that we can learn what it is. We can keep the data as in DataArray or convert that into DataSet before storing them.\n",
    "\n",
    "We start again from loading data using stackstac, but this time we go to next step, clipping the data and computation of snow index, and lets try to save those intermediate result in a zarr file.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5bb65-8143-4841-98f0-a704f1034b2b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis Libraries\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "\n",
    "# Geospatial Data Handling Libraries\n",
    "import geopandas as gpd \n",
    "from shapely.geometry import mapping  \n",
    "import pyproj\n",
    "\n",
    "# Multidimensional and Satellite Data Libraries\n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "import stackstac\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "\n",
    "# Data parallelization and distributed computing libraries\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "\n",
    "# STAC Catalogue Libraries\n",
    "import pystac_client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30439c85-6f76-4747-951f-93612cb6ccbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c711573-b93b-4979-a28c-5e8ce3fb6f84",
   "metadata": {},
   "source": [
    "## Load data using stackstac (with specific chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833d382-e110-4ba8-a911-8c9ce7cb18d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "aoi = gpd.read_file('../data/catchment_outline.geojson', crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(aoi.iloc[0].geometry)\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    intersects=aoi_geojson,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2019-02-01/2019-06-10\"\n",
    ").item_collection()\n",
    "ds = stackstac.stack(items\n",
    "                                ,assets=['green','swir16','scl']\n",
    "                               ,chunksize=( 1, 3, 1024,1024)\n",
    ")\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d282f0-c4f4-493d-9144-292885edb720",
   "metadata": {},
   "source": [
    "## Coomputing Snow index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f2c90-4dc9-4ced-b11a-1a08c3e9316c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "green = ds.sel(band='green')\n",
    "swir = ds.sel(band='swir16')\n",
    "scl = ds.sel(band='scl')\n",
    "ndsi = (green - swir) / (green + swir)\n",
    "snow = xr.where((ndsi > 0.42) & ~np.isnan(ndsi), 1, ndsi)\n",
    "snowmap = xr.where((snow <= 0.42) & ~np.isnan(snow), 0, snow)\n",
    "mask = np.logical_not(scl.isin([8, 9, 3])) \n",
    "snow_cloud = xr.where(mask, snowmap, 2)\n",
    "#snow_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3ce48-30dd-4fc8-8491-1c7124016063",
   "metadata": {},
   "source": [
    "## Clip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939b76f-32f3-48da-bce9-4a082fd0f473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi_utm32 = aoi.to_crs(epsg=32632)\n",
    "geom_utm32 = aoi_utm32.iloc[0]['geometry']\n",
    "snow_cloud.rio.write_crs(\"EPSG:32632\", inplace=True)\n",
    "snow_cloud.rio.set_nodata(np.nan, inplace=True)\n",
    "snow_cloud = snow_cloud.rio.clip([geom_utm32])\n",
    "#snow_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740cd1a-dda2-407b-b58d-e626aa11fb64",
   "metadata": {},
   "source": [
    "## Lets save the intermediate result of a few days in a zarr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86826bf9-baae-45a3-9caf-a0ac784e77bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=snow_cloud.isel(time=slice(0,3))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c06cc-64f1-4a19-9048-d494eaa47a82",
   "metadata": {},
   "source": [
    "## Before saving, we clean the chunk size, clean attribute and save the data in Xarray DataSet format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445791b6-3676-4237-8098-3ff65713c441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=test.chunk(chunks = {'x':'auto', 'y': 'auto'}).to_dataset(name='data')\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987063c7-156a-4d0f-9243-adf6e59a48f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def remove_attrs(obj, to_remove):\n",
    "    new = obj.copy()\n",
    "    new.attrs = {k: v for k, v in obj.attrs.items() if k not in to_remove}\n",
    "\n",
    "    return new\n",
    "\n",
    "def encode(obj):\n",
    "    object_coords = [name for name, coord in obj.coords.items() if coord.dtype.kind == \"O\"]\n",
    "    return obj.drop_vars(object_coords).pipe(remove_attrs, [\"spec\", \"transform\"])\n",
    "\n",
    "\n",
    "test.pipe(encode).to_zarr('test.zarr',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b84245-fc8e-46c9-a531-1e0ccc0bee0d",
   "metadata": {},
   "source": [
    "## Load the temporaly data saved as zarr back to Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e0e9c-c1d3-4744-8e8a-7b8a89db7f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow_cloud=xr.open_zarr('test.zarr').data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7c5c3-7d6e-4618-8207-008915b6fb78",
   "metadata": {},
   "source": [
    "## Group by to a day, and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d939364-683d-4961-bfd7-21031cd53f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date = snow_cloud.groupby(snow_cloud.time.dt.floor('D')).max(skipna=True)\n",
    "clipped_date = clipped_date.rename({'floor': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169af488-e1c7-4791-ab83-9111e0b5978d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date.hvplot.image(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    groupby='date',\n",
    "    crs=pyproj.CRS.from_epsg(32632),\n",
    "    cmap='Pastel2',\n",
    "    clim=(-1, 2),\n",
    "    frame_width=500,\n",
    "    frame_height=500,\n",
    "    title='Snowmap',\n",
    "    geo=True, tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fa945",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Exercise</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>What about saving the data in Netcdf format? `ls -la test.zarr` and  `ls -la test.zarr/nobs `</li>\n",
    "        <li>You can try to explore the zarr file you just created using `ls -la test.zarr` and  `ls -la test.zarr/nobs `</li>\n",
    "        <li>You can explore zarr metadata file by `cat test.zarr/.zmetadata` </li>\n",
    "        <li>Did you find the __chunks__ we defined previously in your zarr file? </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0670d-11cf-4521-abd4-515cb2dd51fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xr.open_zarr('test.zarr').to_netcdf('test.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446367c-4d32-4d0d-8d60-72c3c9d87818",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh test.zarr/ test.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b75a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!ls  -la test.zarr/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0f70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat test.zarr/.zmetadata | head -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe944f5-662d-4f3c-8ae9-66b1c436b764",
   "metadata": {},
   "source": [
    "Zarr format main characteristics are the following:\n",
    "\n",
    "- Every chunk of a Zarr dataset is stored as a single file (see x.y files in `ls -al test.zarr/data`)\n",
    "- Each Data array in a Zarr dataset has a two unique files containing metadata:\n",
    "  - .zattrs for dataset or dataarray general metadatas\n",
    "  - .zarray indicating how the dataarray is chunked, and where to find them on disk or other storage.\n",
    "  \n",
    "Zarr can be considered as an Analysis Ready, cloud optimized data (ARCO) file format, discussed in [data_discovery](./data_discovery.ipynb) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9103092",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Opening multiple NetCDF files and Kerchunk\n",
    "\n",
    "As shown in the [Data discovery](./data_discovery.ipynb) chapter, when we have several files to read at once, we need to use Xarray `open_mfdataset`. When using `open_mfdataset` with NetCDF files, each NetCDF file is considerd as 'one chunk' by default as seen above.\n",
    "\n",
    "When calling `open_mfdataset`, Xarray also needs to analyse each NetCDF file to get metadatas and tried to build a coherent dataset from them. Thus, it performs multiple operations, like concartenate the coordinate, checking compatibility, etc. This can be time consuming ,especially when dealing with object storage or you have more than thousands of files. And this has to be repeated every time, even if we use exactly the same set of input files for different analysis.\n",
    "\n",
    "[Kerchunk library](https://fsspec.github.io/kerchunk/) can build virtual Zarr Dataset over NetCDF files which enables efficient access to the data from traditional file systems or cloud object storage.\n",
    "\n",
    "And that is not the only optimisation kerchunk brings to pangeo ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2feb8d",
   "metadata": {},
   "source": [
    "### Exploiting native file chunks for reading datasets\n",
    "\n",
    "As already mentioned, many data formats (for instance [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), [netCDF4](https://unidata.github.io/netcdf4-python/) with HDF5 backend, [geoTIFF](https://en.wikipedia.org/wiki/GeoTIFF)) have chunk capabilities. Chunks are defined at the creation of each file.  Let's call them '__native file chunks__' to distinguish that from '__Dask chunks__'. These native file chunks can be retrieved and used when opening and accessing the files. This will allow to significantly reduce the amount of IOs, bandwith, and memory usage when analyzing Data Variables.\n",
    "\n",
    "[kerchunk library](https://fsspec.github.io/kerchunk/) can extract native file chunks layout and metadata from each file and combine them into one virtual Zarr dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c231f",
   "metadata": {},
   "source": [
    "### Extract chunk information\n",
    "\n",
    "We extract native file chunk information from each NetCDF file using `kerchunk.hdf`.\n",
    "Let's start with a single file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2798bd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import kerchunk.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2de91",
   "metadata": {},
   "source": [
    "We use `kerchunk.hdf` because our files are written in `netCDF4`  format which is based on HDF5 and `SingleHdf5ToZarr` to translate the metadata of one HDF5 file into Zarr metadata format. The parameter `inline_threshold` is an *optimization* and tells `SingleHdf5ToZarr` to include chunks smaller than this value directly in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e1fab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "remote_filename = 'https://object-store.cloud.muni.cz/swift/v1/foss4g-data/CGLS_LTS_1999_2019/c_gls_NDVI-LTS_1999-2019-1221_GLOBE_VGT-PROBAV_V3.0.1.nc'\n",
    "with fsspec.open(remote_filename) as inf:\n",
    "    h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, remote_filename, inline_threshold=100)\n",
    "    chunk_info = h5chunks.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfa43e",
   "metadata": {},
   "source": [
    "Let's have a look at `chunk_info`. It is a Python dictionary so we can use `pprint` to print it nicely.\n",
    "\n",
    "Content is a bit complicated, but it's only metadata in Zarr format indicating what's in the original file, and where the chunks of the file are located (bytes offset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5ce25",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(chunk_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4ed77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Exercise</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Did you recognise the similarities with test.zarr's zarr metadata file? </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804dffbd",
   "metadata": {},
   "source": [
    "After we have collected information on the native file chunks in the original data file and consolidated our Zarr metadata, we can open the files using `zarr` and pass this chunk information into a storage option. We also need to pass `\"consolidated\": False` because the original dataset does not contain any `zarr` consolidating metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a2a88",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "LTS = xr.open_dataset(\n",
    "    \"reference://\", \n",
    "    engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\": chunk_info,\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb894f3",
   "metadata": {},
   "source": [
    "As you can notice above, all the Data Variables are already chunked according to the native file chunks of the NetCDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13813577",
   "metadata": {},
   "source": [
    "### Combine all LTS files into one kerchunked single ensemble dataset\n",
    "\n",
    "Now we will combine all the files into one kerchunked consolidated dataset, and try to open it as a xarray dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8ac14",
   "metadata": {},
   "source": [
    "Let us first collect the chunk information for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31397e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fs.ls('foss4g-data/CGLS_LTS_1999_2019/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5dfdd",
   "metadata": {},
   "source": [
    "We have 36 files to process, but for this chunking_introduction example, we'll just use 6 file so that it take less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bf519",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada26891",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "s3path = 's3://foss4g-data/CGLS_LTS_1999_2019/c_gls_NDVI-LTS_1999-2019-0[7-8]*.nc'\n",
    "chunk_info_list = []\n",
    "time_list = []\n",
    "\n",
    "for file in fs.glob(s3path):\n",
    "    url = 'https://object-store.cloud.muni.cz/swift/v1/' + file\n",
    "    t = datetime.strptime(file.split('/')[-1].split('_')[3].replace('1999-', ''), \"%Y-%m%d\")\n",
    "    time_list.append(t)\n",
    "    print('working on ', file)\n",
    "    with fsspec.open(url) as inf:\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, url, inline_threshold=100)\n",
    "        chunk_info_list.append(h5chunks.translate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545adc5",
   "metadata": {},
   "source": [
    "This time we use `MultiZarrToZarr` to combine multiple kerchunked datasets into a single logical aggregated dataset. Like when opening multiple files with Xarray `open_mfdataset`, we need to tell `MultiZarrToZarr` how to concatenate all the files. There is no time dimension in the original dataset, but one file corresponds to one date (average over the period 1999-2019 for a given 10-day period e.g. January 01, January 11, January 21, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095a0b1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "mzz = MultiZarrToZarr(\n",
    "    chunk_info_list,\n",
    "    coo_map={'INDEX': 'INDEX'},\n",
    "    identical_dims=['crs'],\n",
    "    concat_dims=[\"INDEX\"],\n",
    ")\n",
    "\n",
    "out = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db85d9b",
   "metadata": {},
   "source": [
    "Then, we can open the complete dataset using our consolidated Zarr metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f8160",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS = xr.open_dataset(\n",
    "    \"reference://\", \n",
    "    engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\": out,\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee93dae",
   "metadata": {},
   "source": [
    "We can save the consolidated metadata for our dataset in a file, and reuse it later to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b9096",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8a98a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "jsonfile='test.json'\n",
    "with open(jsonfile, mode='w') as f :\n",
    "    json.dump(out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7baa9e",
   "metadata": {},
   "source": [
    "We can then load data from this catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1c1a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "LTS = xr.open_dataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\":'./test.json',\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a8397",
   "metadata": {},
   "source": [
    "The catalog (json file we created) can be shared on the cloud (or GitHub, etc.) and anyone can load it from there too.\n",
    "This approach allows anyone to easily access LTS data and select the Area of Interest for their own study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa71c11",
   "metadata": {},
   "source": [
    "We have prepared json file based on 36 netcdf file, and published it online as catalogue=\"https://object-store.cloud.muni.cz/swift/v1/foss4g-catalogue/c_gls_NDVI-LTS_1999-2019.json\"\n",
    "We can try to load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd3626-528e-44dc-aeb0-47f1560f525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue=\"https://object-store.cloud.muni.cz/swift/v1/foss4g-catalogue/c_gls_NDVI-LTS_1999-2019.json\"\n",
    "LTS = xr.open_dataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\":catalogue\n",
    "                    },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50a427-f7ca-497d-b4bf-359b68c56f07",
   "metadata": {},
   "source": [
    "We will use this catalogue in [dask_introduction](./dask_introduction.ipynb) chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36efa13-b1e5-429a-9b2c-537b07d9e49a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Operations on a chunked dataset\n",
    "\n",
    "Let's have a look of our chunked test dataset backend representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302b99f",
   "metadata": {},
   "source": [
    "` test.data` is the backend array Python representation of Xarray's Data Array, [__Dask Array__](https://docs.dask.org/en/stable/array.html) when using chunking, Numpy by default.\n",
    "\n",
    "We will introduce Dask arrays and Dask graphs visualization in the next section [dask_introduction](./dask_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da222df-6f45-434c-b741-34ef27352199",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Anyway, when applying `chunk` function you may have the impression that the chunks sizes just changes and everything will be fine.\n",
    "\n",
    "However, as you can see in the graph visualization above, Xarray will actually have to fetch at least one entire initial chunk that was defined when opening the Dataset at first before rechunking at a smaller size or even selecting one value. This is true when applying any funtions on any values: Xarray will work by loading entire chunks.\n",
    "\n",
    "You can imagine that it will not be very optimal if you load one file as an entire chunk, or if your initial chunks are too big (your Python Jupyter kernel may crash!), especially with large numbers of files and large files. Choosing an appropriate chunk size is really important and depends on your analysis.\n",
    "\n",
    "You can find a really nice article by Dask team on how to chose the right chunk size [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665722c-4c65-4334-987e-1f5bc4841036",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Understanding chunking is key to optimize your data analysis when dealing with big datasets. In this episode we learned how to optimize the data access time and memory resources by exploiting native file chunks from netCDF4 data files and instructing Xarray to access data per chunk. However, computations on big datasets can be very slow on a single computer, and to optimize its time we may need to parallelize your computations. This is what you will learn in the next episode with Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1f23a-8838-41fe-96d7-4f7b0fb9cc3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Key Points</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Chunking </li>\n",
    "        <li>zarr </li>\n",
    "        <li>kerchunk</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c80a4",
   "metadata": {},
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"chunking\" and topic % \"package\"\n",
    ":keyprefix: d-\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
